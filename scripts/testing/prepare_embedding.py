#!/usr/bin/env python3.8
"""
Script chu·∫©n b·ªã d·ªØ li·ªáu cho embedding phase (US-003)
Ki·ªÉm tra v√† validate format d·ªØ li·ªáu ƒë·∫ßu ra t·ª´ pipeline
"""

import json
import sys
import os
import pickle
from pathlib import Path
from datetime import datetime

def validate_embedding_ready_format(file_path):
    """Validate format c·ªßa embedding_ready.json"""
    print(f"üîç Validating embedding_ready format: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Ki·ªÉm tra structure b·∫Øt bu·ªôc
        required_keys = ['documents', 'metadata', 'ids']
        for key in required_keys:
            if key not in data:
                print(f"‚ùå Missing required key: {key}")
                return False
        
        # Ki·ªÉm tra documents
        documents = data['documents']
        if not isinstance(documents, list):
            print("‚ùå 'documents' must be a list")
            return False
        
        if len(documents) == 0:
            print("‚ùå 'documents' list is empty")
            return False
        
        # Ki·ªÉm tra metadata
        metadata = data['metadata']
        if not isinstance(metadata, list):
            print("‚ùå 'metadata' must be a list")
            return False
        
        if len(metadata) != len(documents):
            print(f"‚ùå Metadata length ({len(metadata)}) != documents length ({len(documents)})")
            return False
        
        # Ki·ªÉm tra ids
        ids = data['ids']
        if not isinstance(ids, list):
            print("‚ùå 'ids' must be a list")
            return False
        
        if len(ids) != len(documents):
            print(f"‚ùå IDs length ({len(ids)}) != documents length ({len(documents)})")
            return False
        
        # Ki·ªÉm tra unique IDs
        if len(set(ids)) != len(ids):
            print("‚ùå IDs are not unique")
            return False
        
        # Ki·ªÉm tra content c·ªßa documents
        for i, doc in enumerate(documents):
            if not isinstance(doc, str):
                print(f"‚ùå Document {i} is not a string")
                return False
            
            if len(doc.strip()) == 0:
                print(f"‚ùå Document {i} is empty")
                return False
        
        # Ki·ªÉm tra metadata structure
        for i, meta in enumerate(metadata):
            if not isinstance(meta, dict):
                print(f"‚ùå Metadata {i} is not a dict")
                return False
            
            # Ki·ªÉm tra required metadata fields
            required_meta_fields = ['chunk_id', 'source_file', 'type']
            for field in required_meta_fields:
                if field not in meta:
                    print(f"‚ùå Metadata {i} missing required field: {field}")
                    return False
        
        print(f"‚úÖ Embedding ready format is valid")
        print(f"   - Documents: {len(documents)}")
        print(f"   - Metadata: {len(metadata)}")
        print(f"   - IDs: {len(ids)}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error validating embedding_ready.json: {str(e)}")
        return False

def check_encoding_utf8(file_path):
    """Ki·ªÉm tra encoding UTF-8"""
    print(f"üîç Checking UTF-8 encoding: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"‚úÖ File is properly UTF-8 encoded ({len(content)} characters)")
        return True
        
    except UnicodeDecodeError as e:
        print(f"‚ùå UTF-8 encoding error: {str(e)}")
        return False
    except Exception as e:
        print(f"‚ùå Error checking encoding: {str(e)}")
        return False

def analyze_text_content(embedding_ready_file):
    """Ph√¢n t√≠ch n·ªôi dung text cho embedding"""
    print(f"üìä Analyzing text content for embedding...")
    
    try:
        with open(embedding_ready_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        documents = data['documents']
        metadata = data['metadata']
        
        # Th·ªëng k√™ c∆° b·∫£n
        total_chars = sum(len(doc) for doc in documents)
        total_words = sum(len(doc.split()) for doc in documents)
        avg_chars_per_doc = total_chars / len(documents)
        avg_words_per_doc = total_words / len(documents)
        
        print(f"üìà Text Statistics:")
        print(f"   - Total documents: {len(documents)}")
        print(f"   - Total characters: {total_chars:,}")
        print(f"   - Total words: {total_words:,}")
        print(f"   - Average chars per document: {avg_chars_per_doc:.1f}")
        print(f"   - Average words per document: {avg_words_per_doc:.1f}")
        
        # Ph√¢n t√≠ch content types
        content_types = {}
        for meta in metadata:
            content_type = meta.get('type', 'unknown')
            content_types[content_type] = content_types.get(content_type, 0) + 1
        
        print(f"üìã Content Types:")
        for content_type, count in content_types.items():
            print(f"   - {content_type}: {count} documents")
        
        # Ki·ªÉm tra text quality
        quality_issues = []
        
        for i, doc in enumerate(documents):
            # Ki·ªÉm tra qu√° ng·∫Øn
            if len(doc.strip()) < 50:
                quality_issues.append(f"Document {i} too short ({len(doc)} chars)")
            
            # Ki·ªÉm tra qu√° d√†i
            if len(doc) > 10000:
                quality_issues.append(f"Document {i} too long ({len(doc)} chars)")
            
            # Ki·ªÉm tra special characters (relaxed line break check)
            if doc.count('\n') > 50:  # Increased from 20 to 50 for more lenient checking
                quality_issues.append(f"Document {i} has too many line breaks")
        
        if quality_issues:
            print(f"‚ö†Ô∏è Quality Issues Found:")
            for issue in quality_issues[:5]:  # Show first 5 issues
                print(f"   - {issue}")
            if len(quality_issues) > 5:
                print(f"   ... and {len(quality_issues) - 5} more issues")
        else:
            print("‚úÖ No quality issues found")
        
        return {
            'total_documents': len(documents),
            'total_characters': total_chars,
            'total_words': total_words,
            'avg_chars_per_doc': avg_chars_per_doc,
            'avg_words_per_doc': avg_words_per_doc,
            'content_types': content_types,
            'quality_issues': quality_issues
        }
        
    except Exception as e:
        print(f"‚ùå Error analyzing text content: {str(e)}")
        return None

def create_embedding_config(output_dir):
    """T·∫°o config file cho embedding phase"""
    print(f"‚öôÔ∏è Creating embedding configuration...")
    
    config = {
        "embedding_config": {
            "model_name": "sentence-transformers/all-MiniLM-L6-v2",
            "dimension": 384,
            "batch_size": 32,
            "max_length": 512
        },
        "vector_store_config": {
            "type": "chromadb",
            "collection_name": "fis_internal_docs",
            "persist_directory": "/opt/rag-copilot/vector_store"
        },
        "processing_config": {
            "chunk_overlap": 0,
            "normalize_embeddings": True,
            "remove_duplicates": True
        },
        "created_at": datetime.now().isoformat(),
        "ready_for_us003": True
    }
    
    config_file = Path(output_dir) / "embedding_config.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Embedding config saved: {config_file}")
    return config_file

def main():
    if len(sys.argv) != 2:
        print("S·ª≠ d·ª•ng: python3.8 prepare_embedding.py <final_output_directory>")
        print("V√≠ d·ª•: python3.8 prepare_embedding.py /opt/rag-copilot/output/AI-Starter-Kit_final_output")
        sys.exit(1)
    
    final_output_dir = sys.argv[1]
    
    if not Path(final_output_dir).exists():
        print(f"‚ùå Directory kh√¥ng t·ªìn t·∫°i: {final_output_dir}")
        sys.exit(1)
    
    print("üöÄ PREPARING DATA FOR EMBEDDING PHASE (US-003)")
    print("=" * 60)
    print(f"Final output directory: {final_output_dir}")
    
    # Test files
    embedding_ready_file = Path(final_output_dir) / "embedding_ready.json"
    
    if not embedding_ready_file.exists():
        print(f"‚ùå embedding_ready.json not found: {embedding_ready_file}")
        sys.exit(1)
    
    # Validation steps
    results = {}
    
    # Step 1: Validate format
    results['format_valid'] = validate_embedding_ready_format(embedding_ready_file)
    
    # Step 2: Check encoding
    results['encoding_valid'] = check_encoding_utf8(embedding_ready_file)
    
    # Step 3: Analyze content
    analysis = analyze_text_content(embedding_ready_file)
    results['content_analysis'] = analysis is not None
    
    # Step 4: Create config
    if all(results.values()):
        config_file = create_embedding_config(final_output_dir)
        results['config_created'] = config_file.exists()
    else:
        results['config_created'] = False
    
    # Summary
    print("\n" + "=" * 60)
    print("üìã EMBEDDING PREPARATION SUMMARY")
    print("=" * 60)
    
    for test_name, success in results.items():
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{test_name}: {status}")
    
    overall_success = all(results.values())
    print(f"\nOverall: {'‚úÖ READY FOR US-003' if overall_success else '‚ùå NOT READY'}")
    
    if overall_success:
        print("\nüéâ Data is ready for embedding phase!")
        print("‚úÖ Next steps:")
        print("   1. Run US-003 embedding pipeline")
        print("   2. Use embedding_ready.json as input")
        print("   3. Follow embedding_config.json settings")
        
        # Save preparation report
        report = {
            "timestamp": datetime.now().isoformat(),
            "final_output_dir": final_output_dir,
            "validation_results": results,
            "content_analysis": analysis,
            "ready_for_us003": overall_success
        }
        
        report_file = Path(final_output_dir) / "embedding_preparation_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"   4. Check preparation report: {report_file}")
        
    else:
        print("\nüí• Fix the failed validations before proceeding to US-003")
        sys.exit(1)

if __name__ == "__main__":
    main() 