#!/usr/bin/env python3.8
"""
Script chunking Ä‘Æ¡n giáº£n cho RAG Pipeline
Chia text theo cÃ¢u, khÃ´ng phá»¥ thuá»™c vÃ o headings
"""

import json
import re
import sys
from datetime import datetime

def simple_chunk_text(text, max_tokens=800, min_tokens=200):
    """Chia text thÃ nh chunks theo cÃ¢u"""
    
    # TÃ¡ch cÃ¢u
    sentences = re.split(r'[.!?]+\s+', text)
    
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # Æ¯á»›c tÃ­nh tokens (1 token â‰ˆ 4 chars cho tiáº¿ng Viá»‡t)
        sentence_tokens = len(sentence) // 4
        
        # Náº¿u cÃ¢u quÃ¡ dÃ i, chia nhá» hÆ¡n
        if sentence_tokens > max_tokens:
            # LÆ°u chunk hiá»‡n táº¡i
            if current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_tokens = 0
            
            # Chia cÃ¢u dÃ i thÃ nh cÃ¡c pháº§n
            words = sentence.split()
            temp_chunk = []
            temp_tokens = 0
            
            for word in words:
                word_tokens = len(word) // 4
                if temp_tokens + word_tokens > max_tokens and temp_chunk:
                    chunks.append(' '.join(temp_chunk))
                    temp_chunk = [word]
                    temp_tokens = word_tokens
                else:
                    temp_chunk.append(word)
                    temp_tokens += word_tokens
            
            if temp_chunk:
                chunks.append(' '.join(temp_chunk))
        
        # Náº¿u thÃªm cÃ¢u nÃ y vÆ°á»£t quÃ¡ max_tokens
        elif current_tokens + sentence_tokens > max_tokens:
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_tokens = sentence_tokens
        else:
            current_chunk.append(sentence)
            current_tokens += sentence_tokens
    
    # ThÃªm chunk cuá»‘i
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    # Gá»™p chunks quÃ¡ nhá» vá»›i chunk tiáº¿p theo
    final_chunks = []
    i = 0
    while i < len(chunks):
        chunk = chunks[i]
        chunk_tokens = len(chunk) // 4
        
        if chunk_tokens < min_tokens and i + 1 < len(chunks):
            next_chunk = chunks[i + 1]
            combined_tokens = chunk_tokens + len(next_chunk) // 4
            
            if combined_tokens <= max_tokens:
                final_chunks.append(chunk + ' ' + next_chunk)
                i += 2  # Skip next chunk
            else:
                final_chunks.append(chunk)
                i += 1
        else:
            final_chunks.append(chunk)
            i += 1
    
    return final_chunks

def main():
    if len(sys.argv) != 2:
        print("Sá»­ dá»¥ng: python3.8 simple_chunk.py <processed_file.json>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    try:
        # Äá»c file processed
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        text = data['clean_content']
        original_metadata = data['metadata']
        
        print(f"Äang xá»­ lÃ½ file: {original_metadata['file_name']}")
        print(f"Text length: {len(text)} chars")
        print(f"Estimated tokens: {len(text) // 4}")
        
        # Chunking
        chunks_text = simple_chunk_text(text)
        
        # Táº¡o chunks vá»›i metadata
        chunks = []
        for i, chunk_content in enumerate(chunks_text):
            chunk_tokens = len(chunk_content) // 4
            chunks.append({
                'chunk_id': i + 1,
                'content': chunk_content,
                'tokens': chunk_tokens,
                'chars': len(chunk_content),
                'words': len(chunk_content.split())
            })
        
        # Thá»‘ng kÃª
        total_tokens = sum(chunk['tokens'] for chunk in chunks)
        avg_tokens = total_tokens // len(chunks) if chunks else 0
        min_tokens = min(chunk['tokens'] for chunk in chunks) if chunks else 0
        max_tokens = max(chunk['tokens'] for chunk in chunks) if chunks else 0
        
        stats = {
            'total_chunks': len(chunks),
            'total_tokens': total_tokens,
            'avg_tokens_per_chunk': avg_tokens,
            'min_tokens': min_tokens,
            'max_tokens': max_tokens,
            'processed_time': datetime.now().isoformat()
        }
        
        # Táº¡o káº¿t quáº£
        result = {
            'source_file': input_file,
            'original_metadata': original_metadata,
            'chunks': chunks,
            'stats': stats
        }
        
        # LÆ°u file
        output_file = input_file.replace('_processed.json', '_simple_chunked.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # In káº¿t quáº£
        print(f"\nâœ… Chunking hoÃ n thÃ nh!")
        print(f"ğŸ“Š Thá»‘ng kÃª:")
        print(f"   - Tá»•ng chunks: {stats['total_chunks']}")
        print(f"   - Trung bÃ¬nh: {stats['avg_tokens_per_chunk']} tokens/chunk")
        print(f"   - Min: {stats['min_tokens']} tokens")
        print(f"   - Max: {stats['max_tokens']} tokens")
        print(f"   - Tá»•ng tokens: {stats['total_tokens']}")
        
        print(f"\nğŸ“– Preview 3 chunks Ä‘áº§u:")
        for i, chunk in enumerate(chunks[:3]):
            print(f"\nChunk {i+1} ({chunk['tokens']} tokens):")
            preview = chunk['content'][:150]
            print(f"{preview}{'...' if len(chunk['content']) > 150 else ''}")
        
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u: {output_file}")
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 